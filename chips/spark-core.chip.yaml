# Spark Core Intelligence Chip
# Built-in chip that captures coding and tool patterns

chip:
  id: spark-core
  name: Spark Core Intelligence
  version: 1.0.0
  description: |
    Core intelligence for coding and tool patterns.
    Learns what works in software development workflows.
  author: Vibeship
  license: MIT
  human_benefit: "Improve coding outcomes and debugging speed through reusable patterns."
  harm_avoidance:
    - "No unsafe automation in production"
    - "No misleading or unverified claims"
  risk_level: medium
  safety_tests:
    - "no_unsafe_automation"
    - "no_misleading_claims"
  domains:
    - coding
    - development
    - debugging
    - tools

triggers:
  patterns:
    - "worked because"
    - "failed because"
    - "the issue was"
    - "fixed by"
    - "better approach"
    - "prefer to"
    - "prefer using"
    - "always use"
    - "never use"
    - "remember to"

  events:
    - post_tool
    - post_tool_failure
    - user_prompt
    - PostToolUse
    - PostToolUseFailure
    - UserPromptSubmit

observers:
  - name: success_pattern
    description: Captures when something works and why
    triggers:
      - "worked because"
      - "fixed by"
      - "the solution was"
    capture:
      required:
        pattern: What worked
      optional:
        reason: Why it worked
        context: Surrounding context
    extraction:
      - field: pattern
        patterns:
          - "worked because\\s+(.+)"
          - "fixed by\\s+(.+)"
          - "the solution was\\s+(.+)"
      - field: reason
        patterns:
          - "because\\s+(.+)"

  - name: failure_pattern
    description: Captures when something fails and why
    triggers:
      - "failed because"
      - "the issue was"
      - "the problem was"
    capture:
      required:
        pattern: What failed
      optional:
        reason: Why it failed
        fix: How to fix it
    extraction:
      - field: pattern
        patterns:
          - "failed because\\s+(.+)"
          - "the issue was\\s+(.+)"
          - "the problem was\\s+(.+)"
      - field: fix
        patterns:
          - "fixed by\\s+(.+)"
          - "solution was\\s+(.+)"

  - name: preference
    description: Captures user preferences
    triggers:
      - "prefer to"
      - "prefer using"
      - "better to"
      - "better when"
      - "always use"
      - "never use"
    capture:
      required:
        preference: The preference
      optional:
        reason: Why they prefer it
    extraction:
      - field: preference
        patterns:
          - "prefer\\s+(.+)"
          - "always\\s+(.+)"
          - "never\\s+(.+)"
          - "better\\s+(.+)"
      - field: reason
        patterns:
          - "because\\s+(.+)"

learners:
  - name: tool_effectiveness
    description: Learns which tools work best for which tasks
    type: correlation
    input:
      fields:
        - tool_name
        - task_type
    output:
      fields:
        - success
        - time_taken
    learn:
      - "Which tools succeed most often"
      - "Which tools are fastest"

  - name: error_patterns
    description: Learns common error patterns and fixes
    type: pattern
    observe: failure_pattern
    learn:
      - "Common error types"
      - "Effective fixes"

outcomes:
  positive:
    - condition: "success == true"
      weight: 1.0
      insight: "Approach worked"
    - condition: "time_taken < 30"
      weight: 0.7
      insight: "Fast resolution"

  negative:
    - condition: "success == false"
      weight: 1.0
      insight: "Approach failed"
    - condition: "retry_count > 2"
      weight: 0.8
      insight: "Multiple retries needed"

questions:
  - id: core_stack
    question: What is the primary tech stack (language, framework, tools)?
    category: goal
    affects_learning:
      - tool_effectiveness
      - error_patterns

  - id: core_quality
    question: What quality signals matter most (tests passing, no lint errors, type safety)?
    category: quality
    affects_learning:
      - success_pattern

  - id: core_done
    question: How do we know a task is truly done (just works, or also tested/documented)?
    category: done
    affects_learning:
      - success_pattern

  - id: core_patterns
    question: What coding patterns or conventions should we follow?
    category: insight
    affects_learning:
      - preference

  - id: core_antipatterns
    question: What should we avoid (anti-patterns, risky approaches)?
    category: risk
    affects_learning:
      - failure_pattern

  - id: core_debug
    question: What debugging approach works best for this codebase?
    category: insight
    phase: prototype
    affects_learning:
      - error_patterns

context:
  format: |
    ## Spark Core Intelligence

    ### Tool Effectiveness
    {{#each tool_insights}}
    - {{tool}}: {{effectiveness}}
    {{/each}}

    ### Error Patterns
    {{#each error_patterns}}
    - {{pattern}}: {{fix}}
    {{/each}}

  priority: 0.9
  max_chars: 500
