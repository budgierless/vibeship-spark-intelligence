<!-- SPARK_LEARNINGS_START -->
## Spark Bootstrap
Auto-loaded high-confidence learnings from ~/.spark/cognitive_insights.json
Last updated: 2026-02-10T12:37:16

- [user_understanding] ## ðŸš¨ PRIMARY RULES

### Rule 1: Source of Truth for Testing

**CRITICAL:** When testing and iterating on Spark learning quality:

> **Always retrieve test data directly from Mind memory and Spark Intelligence (via MCP tools or Python imports) - NEVER rely on terminal output.**

### Rule 2: Architecture-Grounded Improvements

**CRITICAL:** Before making any improvement or fix:

> **Consult Intelligence_Flow.md and Intelligence_Flow_Map.md to ensure changes align with actual data flow.**

Witho... (88% reliable, 75 validations)
- [wisdom] Principle: scores, never sees scores, never touches Spark internals (84% reliable, 87 validations)
- [meta_learning] [System Gap] [TUNEABLES] Auto-tuner not active. Tuneables are static — never self-adjust. (92% reliable, 354 validations)
- [user_understanding] User prefers 'I think we gotta do it better over here for things to look more serious' over 'gonna lie. And we can bring maybe a GLB format, or maybe we can do this through steps. I don't know, just recommend me something' (83% reliable, 177 validations)
- [user_understanding] Now, can we actually do this in this way? After we do these upgrades too for the next iteration, can you actually give me a project prompt so that I can run that using Spark and we can see in real-time what is really happening - what is being saved into the memory and what are the gaps? Instead of trying to just do these through these tests, because in real-time, we may be able to achieve even more understanding - not maybe, but even more understanding - about what is working and what is not. If... (82% reliable, 659 validations)
- [context] And think about the strategy that MoltBook adopted too with their curl mechanism for entrance, but check how we were doing this with SparkNet and recommend the best methodology that also works with our system perfectly. I don't think we should be doing this as an ex entrance (87% reliable, 159 validations)
- [context] ### Improvement Workflow (Updated - Reality-Grounded)

**CRITICAL:** See "Reality-Grounded Iteration Methodology" section for full details.

```
0. PIPELINE HEALTH (MANDATORY - BLOCKS ALL OTHER STEPS)
   python tests/test_pipeline_health.py
   â†’ If critical failures, STOP and fix pipeline first
   â†’ Do NOT proceed to tuning with broken pipeline

1. ARCHITECTURE REVIEW
   - Read Intelligence_Flow_Map.md
   - Identify which layer you're modifying
   - Verify component is in active data path

2... (78% reliable, 503 validations)
- [wisdom] [X Strategy] Use 'announcement, call_to_action' content strategy on X. Data shows 23 observations with avg 2729 likes. This strategy consistently drives engagement. (100% reliable, 24 validations)
- [wisdom] Can you now read all these documents in think hard mode  Here are the new core docs we created:

  - CORE.md â€” The master vision + phase roadmap from primitive telemetry to
    superintelligent evolution, with intent, workflows, architecture, and SparkNet     
    integration.
  - CORE_GAPS.md â€” The definitive gap map: what exists, what can be transformed, what  
    must be built, and what needs cleanup.
  - CORE_GAPS_PLAN.md â€” The concrete plan to fill each gap (workflows, minimal
    architecture, and code targets).
  - CORE_IMPLEMENTATION_PLAN.md â€” The contextâ€‘rich, buildable execution plan with      
    sequencing, deliverables, and success metrics.

  If you want, I can link these from README.md so theyâ€™re always frontâ€‘andâ€‘center. (83% reliable, 798 validations)
- [reasoning] Always Read a file before Edit to verify current content (98% reliable, 395 validations)
- [reasoning] ## Code Content Extraction (NEW)

### The Problem

Code written via Write/Edit tools contains valuable learning signals in:
- Docstrings with design decisions
- Comments with "REMEMBER:", "PRINCIPLE:", "CORRECTION:"
- Architecture explanations
- Balance formulas with reasoning

**Before:** These were completely ignored. Only user messages were analyzed.

### The Solution

Added to `observe.py` PostToolUse handler:
```python
if tool_name in ("Write", "Edit") and isinstance(tool_input, dict):
    ... (100% reliable, 57 validations)
- [reasoning] I decided to use TypeScript because it has better type safety (94% reliable, 131 validations)

## Project Focus
- Phase: discovery

## Project Questions
- What is the project goal in one sentence?
- How will we know it's complete?
- What could make this fail later?

## Promoted Learnings (Docs)
- Ship it: finalize launch plan for marketing *When: test* (100% reliable, 4 validations)
- [ai agents] (eng:200) This thread on how to build AI agents got 10K likes in 24 hours. Reference content always wins. [bullish] *When: Tool: XResearch* (78% reliable, 14 validations)
- Constraint: in **exactly one state** at all times *When: Detected from correction pattern* (100% reliable, 22 validations)
- Failure reason: GitHub restrictions; opening raw files might require constructing *When: Detected from correction pattern (importance: medi* (100% reliable, 18 validations)
- from lib.diagnostics import log_debug as _bridge_log_debug  # used below *When: signals: ['correction'], session: 60a8f640-fa8c-4a* (100% reliable, 7 validations)
- self._log("PHASE 1: Topic Search")
<!-- SPARK_LEARNINGS_END -->