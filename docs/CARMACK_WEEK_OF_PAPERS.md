# Carmack “week of papers” — link sheet (Spark notes)

This is a best-effort mapping from the titles you pasted to canonical paper pages (usually arXiv).

If you can paste the original Carmack post/link, I can make this 100% exact (a few of the titles below are hard to uniquely resolve without search).

---

## Confirmed / high confidence matches

### 15) DreamerV3 — Mastering Diverse Domains through World Models
- Paper: **DreamerV3: Mastering Diverse Domains through World Models**
- arXiv: https://arxiv.org/abs/2301.04104

### 14) Mastering Atari with Discrete World Models
- Paper: **Mastering Atari with Discrete World Models**
- arXiv: https://arxiv.org/abs/2010.02193

### 13) Dream to Control: Learning Behaviors by Latent Imagination
- Paper: **Dream to Control: Learning Behaviors by Latent Imagination**
- arXiv: https://arxiv.org/abs/1912.01603

### 12) Learning Latent Dynamics for Planning from Pixels
- Paper: **Learning Latent Dynamics for Planning from Pixels** (PlaNet)
- arXiv: https://arxiv.org/abs/1811.04551

### 3) Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture
- Paper: **Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture** (I-JEPA)
- arXiv: https://arxiv.org/abs/2301.08243

---

## Unresolved (need the exact Carmack link or permission to web-search)

### 11) Discovering state-of-the-art reinforcement learning algorithms
- Likely a meta-learning / automated algorithm discovery paper.
- Candidates to verify once we have the source link:
  - “Discovering Reinforcement Learning Algorithms”
  - “Discovering Policy Gradient Methods” / “Learning to learn RL updates”

### 10) LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics
- Looks like a JEPA-family paper (LeCun/FAIR line). Need exact PDF.

### 9) floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL
- Flow-matching critics paper; need exact arXiv entry.

### 8) Beyond Gradient Averaging in Parallel Optimization: Improved Robustness through Gradient Agreement Filtering
- Distributed optimization paper; need exact arXiv entry.

### 7) Cautious Weight Decay
- Optimizer/regularization paper; need exact arXiv entry.

### 6) Local Feature Swapping for Generalization in Reinforcement Learning
- RL generalization augmentation paper; need exact arXiv entry.

### 5) Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful
- LM training dynamics paper; need exact arXiv entry.

### 4) Patches Are All You Need?
- Paper: **Patches Are All You Need?**
- arXiv: https://arxiv.org/abs/2201.09792

### 2) Deep Delta Learning
- Paper: **Deep Delta Learning**
- arXiv: https://arxiv.org/abs/2601.00417

### 1) Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning
- Need exact match.

---

## Next step

Paste the link to the Carmack post/article (or just the author list / venue for the unresolved ones) and I’ll:
1) resolve every title to the exact paper URL
2) add a 1-paragraph “why it matters for Spark” summary per paper (optimization angles: memory compression, world models, credit assignment, batching/optimizer stability)
3) generate a “what to implement” shortlist (Spark-friendly, low-risk)
