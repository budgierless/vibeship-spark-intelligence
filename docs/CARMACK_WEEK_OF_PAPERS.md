# Carmack "week of papers" - link sheet (Spark notes)

This is a best-effort mapping from the titles you pasted to canonical paper pages (usually arXiv).

If you can paste the original Carmack post/link, we can verify ordering and exact wording.

---

## Confirmed / high confidence matches

### 15) DreamerV3 - Mastering Diverse Domains through World Models
- Paper: DreamerV3: Mastering Diverse Domains through World Models
- arXiv: https://arxiv.org/abs/2301.04104

### 14) Mastering Atari with Discrete World Models
- Paper: Mastering Atari with Discrete World Models
- arXiv: https://arxiv.org/abs/2010.02193

### 13) Dream to Control: Learning Behaviors by Latent Imagination
- Paper: Dream to Control: Learning Behaviors by Latent Imagination
- arXiv: https://arxiv.org/abs/1912.01603

### 12) Learning Latent Dynamics for Planning from Pixels
- Paper: Learning Latent Dynamics for Planning from Pixels (PlaNet)
- arXiv: https://arxiv.org/abs/1811.04551

### 3) Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture
- Paper: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (I-JEPA)
- arXiv: https://arxiv.org/abs/2301.08243

---

## Lower confidence matches (verify against Carmack's original post if needed)

### 11) Discovering state-of-the-art reinforcement learning algorithms
- Paper: Discovering state-of-the-art reinforcement learning algorithms
- Nature: https://www.nature.com/articles/s41586-025-09761-x

### 10) LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics
- Paper: LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics
- arXiv: https://arxiv.org/abs/2511.08544

### 9) floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL
- Paper: floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL
- arXiv: https://arxiv.org/abs/2509.06863

### 8) Beyond Gradient Averaging in Parallel Optimization: Improved Robustness through Gradient Agreement Filtering
- Paper: Beyond Gradient Averaging in Parallel Optimization: Improved Robustness through Gradient Agreement Filtering
- arXiv: https://arxiv.org/abs/2412.18052

### 7) Cautious Weight Decay
- Paper: Cautious Weight Decay
- arXiv: https://arxiv.org/abs/2510.12402

### 6) Local Feature Swapping for Generalization in Reinforcement Learning
- Paper: Local Feature Swapping for Generalization in Reinforcement Learning
- arXiv: https://arxiv.org/abs/2204.06355

### 5) Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful
- Paper: Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful
- arXiv: https://arxiv.org/abs/2507.07101

### 4) Patches Are All You Need?
- Paper: Patches Are All You Need?
- arXiv: https://arxiv.org/abs/2201.09792

### 2) Deep Delta Learning
- Paper: Deep Delta Learning
- arXiv: https://arxiv.org/abs/2601.00417

### 1) Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning
- Paper: Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning
- arXiv: https://arxiv.org/abs/2512.20605

---

## Next step

Paste the link to the Carmack post/article (or just the author list / venue for the lower-confidence ones) and I'll:
1. Verify every title matches the exact paper URL.
2. Add a short "why it matters for Spark" summary per paper (optimization angles: memory compression, world models, credit assignment, batching/optimizer stability).
3. Generate a "what to implement" shortlist (Spark-friendly, low-risk).
